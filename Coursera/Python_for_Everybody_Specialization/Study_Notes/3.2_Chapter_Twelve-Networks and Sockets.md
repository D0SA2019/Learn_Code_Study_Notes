# 3.2 Chapter 12 - Networks and Sockets

## Transport Control Protocol (TCP)
Built on top IP (Internet Protocol)

Assumes IP might lose some data stores and retransmits data if it seems to be lost

Handles "flow control" using a transmit window

Provides a nice reliable pipe

![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/IP_stack_connections_%28corrected%29.svg/2000px-IP_stack_connections_%28corrected%29.svg.png)

### TCP Connections / Sockets
The network protocol that powers the web is actually quite simple and there is built-in support in Python called `sockets` which makes it very easy to make network connections and retrieve data over those sockets in a Python program.

A socket is much like a file, except that a single socket provides a two-way connection between two programs. You can both read from and write to the same socket. If you write something to a socket, it is sent to the application at the other end of the socket. If you read from the socket, you are given the data which the other application has sent.

But if you try to read a socket when the program on the other end of the socket has not sent any data, you just sit and wait. If the programs on both ends of the socket simply wait for some data without sending anything, they will wait for a very long time, so an important part of programs that communicate over the Internet is to have some sort of protocol.

A protocol is a set of precise rules that determine who is to go first, what they are to do, and then what the responses are to that message, and who sends next, and so on. In a sense the two applications at either end of the socket are doing a dance and making sure not to step on each other's toes.

"In computer networking, an Internet socket or network socket is an endpoint of a bidirectional inter-process communication flow across an Internet Protocol-based computer network, such as the Internet."

Process -> Internet -> Process

### TCP Port Numbers
A port is an application-specific or process-specific software communications endpoint

It allows multiple networked applications to coexist to coexist on the same server.

There is a list of well-known TCP port numbers

![](http://i66.tinypic.com/2ldgcvl.png)

We're going to play within this class the most is port 80. Port 80 is the web port because that's the part that connected to the server. Port 443 is the secure HTTPS.

### Common TCP Ports

* Telnet (23) - Login
* SSH (22) - Secure Login
* HTTP (80)
* HTTPS (443) - Secure
* SMTP (25) - Mail
* IMAP (143/220/993) - Mail Retrieval
* POP (109/110) - Mail Revrieval
* DNS (53) - Domain Name
* FTP (21) - File Transfer

Sometimes we see the port number in the URL if the web server is running on a "non-standart" port.

![](http://i64.tinypic.com/14v7nu1.png)


### Sockets in Python
Python has built-in support for TCP Sockets

```python
import socket
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(("data.pr4e.org", 80))
```


## Aplication Protocol
Since TCP (and Python) gives us a reliable socket, what do we want to do with the socket? What problem do we want to solve?

Application Protocols
* Mail
* World Wide Web

### HTTP - Hypertext Transfer Protocol
* The dominant Application Layer Protocol on the Internet
* Invented for the Web - to Retrieve HTML, Images, Documents, etc
* Extended to be data in addition to documents - RSS, Web Services, etc. Basic Concept - Make a Connection - Request a document - Retrieve the Document - Close the Connection

### What is a Protocol?
A set of rules that all parties follow so we can predict each orhet's behavior

And not bump into each other
* On wto-way roads in USA, drive on the right-hand side of the road
* On two-way roads in the UK drive on the left-hand side of the road

![](http://i68.tinypic.com/vzc5cw.png)

### Getting Data From The server
Each the user clicks on an anchor tag with an href= value to switch to a new page, the browser makes a connection to the web server and issues a "GET" request - to GET the content of the page at the specified URL.

The server returns the HTML document to the browser wich formats and displays the document to the user.

**Request Response Cycle**

![](http://i65.tinypic.com/6sw8m1.png)

### Internet Standarts
The standarts for all of the Internet protocols (inner workings) are developed by an organizaton Internet Engineering Task Force [(IETF)](http://www.ietf.org)

Standarts are colled "RFCs" - "Request for Comments"

![](http://i68.tinypic.com/2zey1xz.png)

![](http://i66.tinypic.com/2dtdtls.png)

![](http://i64.tinypic.com/2lwobaf.png)

```
Heval:~ hevalhazalkurt$ telnet data.pr4e.org 80
Trying 192.241.136.170...
Connected to data.pr4e.org.
Escape character is '^]'.
GET http://data.pr4e.org/page1.htm HTTP/1.0

HTTP/1.1 200 OK
Date: Tue, 08 Jan 2019 04:01:00 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 11:11:47 GMT
ETag: "80-54f8e1f004857"
Accept-Ranges: bytes
Content-Length: 128
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/html

<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://data.pr4e.org/page2.htm">
Second Page</a>.
</p>
Connection closed by foreign host.
Heval:~ hevalhazalkurt$
```

Nmap in the movies : https://nmap.org/movies/

### An HTTP Request in Python

```python
import socket

mysock = socket.socket(socket.AF_INET, socket.SOCT_STREAM)
mysock.connect(("data.pr4e.org", 80))
cmd = "Get http://data.pr4e.org/romeo.txt HTTP/1.0\r\n\r\n".encode()
mysock.send(cmd)

while True:
	data = mysock.recv(512)
	if (len(data) <1):
		break
	print(data.decode())
mysock.close()

# Output :
HTTP/1.1 501 Not Implemented
Date: Tue, 08 Jan 2019 04:11:27 GMT
Server: Apache/2.4.18 (Ubuntu)
Allow: GET,HEAD,POST,OPTIONS
Content-Length: 279
Connection: close
Content-Type: text/html; charset=iso-8859-1

<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>501 Not Implemented</title>
</head><body>
<h1>Not Implemented</h1>
<p>Get to /romeo.txt not supported.<br />
</p>
<hr>
<address>Apache/2.4.18 (Ubuntu) Server at data.pr4e.org Port 80</address>
</body></html>
```



### Worked Exercise : `socket1.py`
Sample Codes download : https://py4e.com/materials.php

```python
import socket

mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect(('data.pr4e.org', 80))
cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\r\n\r\n'.encode()
mysock.send(cmd)

while True:
    data = mysock.recv(512)
    if len(data) < 1:
        break
    print(data.decode(),end='')

mysock.close()

# Output :
HTTP/1.1 200 OK
Date: Tue, 08 Jan 2019 04:22:31 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
```

## Retrieving an image over HTTP
In the above example, we retrieved a plain text file which had newlines in the file and we simply copied the data to the screen as the program ran. We can use a similar program to retrieve an image across using HTTP. Instead of copying the data to the screen as the program runs, we accumulate the data in a string, trim off the headers, and then save the image data to a file as follows:

```python
import socket
import time

HOST = 'data.pr4e.org'
PORT = 80
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect((HOST, PORT))
mysock.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\r\n\r\n')
count = 0
picture = b""

while True:
    data = mysock.recv(5120)
    if len(data) < 1: break
    #time.sleep(0.25)
    count = count + len(data)
    print(len(data), count)
    picture = picture + data

mysock.close()

# Look for the end of the header (2 CRLF)
pos = picture.find(b"\r\n\r\n")
print('Header length', pos)
print(picture[:pos].decode())

# Skip past the header and save the picture data
picture = picture[pos+4:]
fhand = open("stuff.jpg", "wb")
fhand.write(picture)
fhand.close()
```

Output :

```
1448 1448
1448 2896
1448 4344
1448 5792
1448 7240
1448 8688
2896 11584
...
376 230608
Header length 394
HTTP/1.1 200 OK
Date: Wed, 13 Mar 2019 15:56:41 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
```

You can see that for this url, the `Content-Type` header indicates that body of the document is an image (`image/jpeg`). Once the program completes, you can view the image data by opening the file `stuff.jpg` in an image viewer.

As the program runs, you can see that we don't get 5120 characters each time we call the `recv()` method. We get as many characters as have been transferred across the network to us by the web server at the moment we call `recv()`. In this example, we either get as few as 3200 characters each time we request up to 5120 characters of data.

Your results may be different depending on your network speed. Also note that on the last call to `recv()` we get 3167 bytes, which is the end of the stream, and in the next call to `recv()` we get a zero-length string that tells us that the server has called `close()` on its end of the socket and there is no more data forthcoming.

We can slow down our successive `recv()` calls by uncommenting the call to `time.sleep()`. This way, we wait a quarter of a second after each call so that the server can "get ahead" of us and send more data to us before we call `recv()` again. With the delay, in place the program executes as follows:

## Using the Developer Console to Explore HTTP
Go to URL -> Open the browser's Developer Tools

![](http://i65.tinypic.com/npid6r.png)

![](http://i66.tinypic.com/24fldtd.png)

![](http://i65.tinypic.com/ilxh0g.png)

You get these exact same headers either in Python or with Telnet or with the developer console. When you connect with the same web address by Telnet and use the `GET` command you'll see the line `HTTP/1.1 200 OK` like the picture below.

![](http://i66.tinypic.com/5baxx.png)

On the "Response" tab :

![](http://i64.tinypic.com/2pzbhpg.png)

Go to the : https://www.wa4e.com/code/arrays/guess.php?guess=12
You'll see the PHP questioning on the "Header" tab

![](http://i68.tinypic.com/aenvbl.png)


## Unicode Characters and Strings
Computers don't understand letters actually, they understand numbers so we had to come up with a mapping between letters and numbers. The most common mapping of the 1980s is this mapping called ASCII, The American Standart Code for Information Interchange.

![](https://cdn.sparkfun.com/assets/home_page_posts/2/1/2/1/ascii_table_black.png)

### Representing Simple Strings

Each character is represented by a number between 0 and 256 stored in 8 bits of memory.

We refer to "8 bits of memory as a **byte** of memory - (i.e. my disk drive contains 3 tera**bytes** of memory)"

The `ord()` function tells us the numeric value of a simple ASCII character.


```python
print(ord("H"))
print(ord("e"))
print(ord("\n"))


# OUTPUT
72
101
10
```

Unicode is an universal code for hundreds of millions of different characters in different languages.

![](http://i66.tinypic.com/4ufo6b.png)
[link](http://unicode.org/charts/)

### Multi-Byte Characters
To represent the wide range of characters computers must handle we represent characters with more than one byte.

* UTF-16 , Fixed lenght, Two bytes
* UTF-32 , Fixed lenght, Four bytes
* UTF-8 , I-4 bytes
	* Upwards compatible with ASCII
	* Automatic detection between ACII and UTF-8
	* UTF-8 is recommended practice for encoding data to be exchanged between systems.

UTF-32 is a gigantic one and it's 4 times as much data for a single character.

UTF-8 is the best because it overlaps with ASCII.

We need to learn about character sets in network connection because network connections and databases not inside your computer. And something that's changed from Python2 to Python3. There were two kinds of strings in Python : old string and Unicode string.

![](http://i65.tinypic.com/1tovaf.png)

![](http://i68.tinypic.com/vhlvn.png)

In Python3, all strings internally are UNICODE. Working with string variables in Python programs and reading data from files usually "just works". When we talk to a network resource using sockets or talk to a database we have to encode and decode data (usually to UTF-8).

```python
x = b"abc"
print(type(x))
# <class 'bytes'>

x = "もしもし"
print(type(x))
# <class 'str'>

x = u"もしもし"
print(type(x))
# <class 'str'>
```

If we talk to a network, we have to understand. The key thing is we have to decode this stuff and have to realize what is the character set of the stuff we're pulling in. The beauty is most of networks use UTF-8 so it's relatively simple. So there is a little decode operation.

### Python Strings to Bytes

When we talk to an external resource like a network socket we sends byes, so we need to encode Python 3 strings into a given character encoding

When we read data from an external resource, we must decode it based on the character set so it is properly represented in Python 3 as a string

```python
while True:
	data = mysock.recv(512)
	if ( len(data) < 1 ) :
		break
	mystring = data.decode()
	print(mystring)
```

You can tell what character set it is but by default it assumes UTF-8 or ASCII dynamically.


## Retriving Web Pages
### Using `urllib` in Python
While we can manually send and receive data over HTTP using the socket library, there is a much simpler way to perform this common task in Python by using the `urllib` library.

Using `urllib`, you can treat a web page much like a file. You simply indicate which web page you would like to retrieve and `urllib` handles all of the HTTP protocol and header details.

The equivalent code to read the *romeo.txt* file from the web using urllib is as follows:

```python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen("http://data.pr4e.org/romeo.txt")
for line in fhand:
	print(line.decode().strip())
```
Once the web page has been opened with `urllib.urlopen`, we can treat it like a file and read through it using a `for` loop.

When the program runs, we only see the output of the contents of the file. The headers are still sent, but the `urllib` code consumes the headers and only returns the data to us.

Output :
```
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
```

As an example, we can write a program to retrieve the data for *romeo.txt* and compute the frequency of each word in the file as follows:

```python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen("http://data.pr4e.org/romeo.txt")

counts = dict()
for line in fhand:
	words = line.decode().split()
	for word in words:
		counts[word] = counts.get(word, 0) + 1
print(counts)
```
Again, once we have opened the web page, we can read it like a local file.

Output :
```
{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}
```


### Reading Web Pages

```python
import urllib.request, urllib.parse, urllib.error

fhand = urllib.request.urlopen("http://www.dr-chuck.com/page1.htm")

for line in fhand:
	print(line.decode().strip())
```

Output :

```
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
```

## Parsing Web Pages
### What is Web Scraping?
One of the common uses of the `urllib` capability in Python is to scrape the web. Web scraping is when we write a program that pretends to be a web browser and retrieves pages, then examines the data in those pages looking for patterns.

As an example, a search engine such as Google will look at the source of one web page and extract the links to other pages and retrieve those pages, extracting links, and so on. Using this technique, Google spiders its way through nearly all of the pages on the web.

Google also uses the frequency of links from pages it finds to a particular page as one measure of how "important" a page is and how high the page should appear in its search results.

### Why Scrape?
Pull data - particularly social data - who links to who?

Get your own data back out of some system that has no "export capability"

Monitor a site for new information

Spider the web to make a database for a search engine

There is some controversy about web page scraping and some sites are a bit snippy about it.

Republishing copyrighted information is not allowed.

Violating terms of service is not allowed.


### Parsing HTML using regular expressions
One simple way to parse HTML is to use regular expressions to repeatedly search for and extract substrings that match a particular pattern.

Here is a simple web page:

```html
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
```
We can construct a well-formed regular expression to match and extract the link values from the above text as follows:

`href="http[s]?://.+?"`

Our regular expression looks for strings that start with "href="http://" or "href="https://", followed by one or more characters (`.+?`), followed by another double quote. The question mark behind the `[s]?` indicates to search for the string "http" followed by zero or one "s".

The question mark added to the `.+?` indicates that the match is to be done in a "non-greedy" fashion instead of a "greedy" fashion. A non-greedy match tries to find the smallest possible matching string and a greedy match tries to find the largest possible matching string.

We add parentheses to our regular expression to indicate which part of our matched string we would like to extract, and produce the following program:

```Python
# Search for lines that start with From and have an at sign
import urllib.request, urllib.parse, urllib.error
import re
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
html = urllib.request.urlopen(url).read()
links = re.findall(b'href="(http[s]?://.*?)"', html)
for link in links:
    print(link.decode())
```

The `ssl` library allows this program to access web sites that strictly enforce HTTPS. The `read` method returns HTML source code as a bytes object instead of returning an HTTPResponse object. The `findall` regular expression method will give us a list of all of the strings that match our regular expression, returning only the link text between the double quotes.

When we run the program and input a URL, we get the following output:

```
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
```

Regular expressions work very nicely when your HTML is well formatted and predictable. But since there are a lot of "broken" HTML pages out there, a solution only using regular expressions might either miss some valid links or end up with bad data.

This can be solved by using a robust HTML parsing library.


### The Easy Way - Beautiful Soup
Even though HTML looks like XML1 and some pages are carefully constructed to be XML, most HTML is generally broken in ways that cause an XML parser to reject the entire page of HTML as improperly formed.

There are a number of Python libraries which can help you parse HTML and extract data from the pages. Each of the libraries has its strengths and weaknesses and you can pick one based on your needs.

As an example, we will simply parse some HTML input and extract links using the `BeautifulSoup` library. BeautifulSoup tolerates highly flawed HTML and still lets you easily extract the data you need. You can download and install the BeautifulSoup code from:: https://pypi.org/project/beautifulsoup4/

```python
import urllib.request, urllib.parse, urllib.error
from bs4 import BeautifulSoup
import ssl

# ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input("Enter - ")
html = urllib.request.urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, "html.parser")

tags = soup("a")
for tag in tags:
	print(tag.get("href", None))
```

The program prompts for a web address, then opens the web page, reads the data and passes the data to the BeautifulSoup parser, and then retrieves all of the anchor tags and prints out the `href` attribute for each tag.

When the program runs, it produces the following outputs for different websites:

```
Enter - http://www.dr-chuck.com/page1.htm
http://www.dr-chuck.com/page2.htm
```

```
Enter - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.6.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
bugs.html
http://sphinx.pocoo.org/
```
This list is much longer because some HTML anchor tags are relative paths (e.g., tutorial/index.html) or in-page references (e.g., '#') that do not include "http://" or "https://", which was a requirement in our regular expression.

You can use also BeautifulSoup to pull out various parts of each tag:

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
import ssl

# Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter - ')
html = urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, "html.parser")

# Retrieve all of the anchor tags
tags = soup('a')
for tag in tags:
    # Look at the parts of a tag
    print('TAG:', tag)
    print('URL:', tag.get('href', None))
    print('Contents:', tag.contents[0])
    print('Attrs:', tag.attrs)
```

Output :

```
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Contents:
Second Page
Attrs: {'href': 'http://www.dr-chuck.com/page2.htm'}
```


`html.parser` is the HTML parser included in the standard Python 3 library. Information on other HTML parsers is available at:

http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser

These examples only begin to show the power of BeautifulSoup when it comes to parsing HTML.
